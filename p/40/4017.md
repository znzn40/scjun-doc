---
manual:WebAPI
version:0
lang:zh
rawUrl:https://developer.mozilla.org/zh-CN/docs/Web/API/BaseAudioContext/createAnalyser
---





[`AudioContext`]2544 "AudioContext接口表示由音频模块连接而成的音频处理图，每个模块对应一个AudioNode。AudioContext可以控制它所包含的节点的创建，以及音频处理、解码操作的执行。做任何事情之前都要先创建AudioContext对象，因为一切都发生在这个环境之中。")的`createAnalyser()`方法能创建一个[`AnalyserNode`]2531 "AnalyserNode 赋予了节点可以提供实时频率及时间域分析的信息。它使一个 AudioNode 通过音频流不做修改的从输入到输出, 但允许你获取生成的数据, 处理它并创建音频可视化.")，可以用来获取音频时间和频率数据，以及实现数据可视化。




**注意**：关于该节点的更多信息，请查看[`AnalyserNode`]2531 "AnalyserNode 赋予了节点可以提供实时频率及时间域分析的信息。它使一个 AudioNode 通过音频流不做修改的从输入到输出, 但允许你获取生成的数据, 处理它并创建音频可视化.")



## 语法<a name="语法"></a>

```
var audioCtx = new AudioContext();
var analyser = audioCtx.createAnalyser();
```

### 返回值<a name="Description"></a>


[`AnalyserNode`]2531 "AnalyserNode 赋予了节点可以提供实时频率及时间域分析的信息。它使一个 AudioNode 通过音频流不做修改的从输入到输出, 但允许你获取生成的数据, 处理它并创建音频可视化.")对象


## 例子<a name="Examples"></a>


下面的例子展示了AudioContext创建分析器节点的基本用法，然后用requestAnimationFrame()来反复获取时域数据，并绘制出当前音频输入的“示波器风格”输出。更多完整例子请查看[Voice-change-O-matic]3430 "")demo (中[app.js的128–205行]3431 "")代码)


```
var audioCtx = new (window.AudioContext || window.webkitAudioContext)();
var analyser = audioCtx.createAnalyser();

  ...

analyser.fftSize = 2048;
var bufferLength = analyser.fftSize;
var dataArray = new Uint8Array(bufferLength);
analyser.getByteTimeDomainData(dataArray);

// draw an oscilloscope of the current audio source

function draw() {

      drawVisual = requestAnimationFrame(draw);

      analyser.getByteTimeDomainData(dataArray);

      canvasCtx.fillStyle = 'rgb(200, 200, 200)';
      canvasCtx.fillRect(0, 0, WIDTH, HEIGHT);

      canvasCtx.lineWidth = 2;
      canvasCtx.strokeStyle = 'rgb(0, 0, 0)';

      canvasCtx.beginPath();

      var sliceWidth = WIDTH * 1.0 / bufferLength;
      var x = 0;

      for(var i = 0; i < bufferLength; i++) {
   
        var v = dataArray[i] / 128.0;
        var y = v * HEIGHT/2;

        if(i === 0) {
          canvasCtx.moveTo(x, y);
        } else {
          canvasCtx.lineTo(x, y);
        }

        x += sliceWidth;
      }

      canvasCtx.lineTo(canvas.width, canvas.height/2);
      canvasCtx.stroke();
    };

    draw();
```

## 规范<a name="规范"></a>
Specification | Status | Comment 
[Web Audio API<br></br><small>createAnalyser()</small>]22913 "") | Working Draft |  


## 浏览器兼容性<a name="浏览器兼容性"></a>


**[We&#39;re converting our compatibility data into a machine-readable JSON format]3344 "")**. This compatibility table still uses the old format, because we haven&#39;t yet converted the data it contains.**[Find out how you can help!]3392 "")**


* 
* 
Feature | Chrome | Firefox (Gecko) | Internet Explorer | Opera | Safari (WebKit) 
Basic support | 10.0[webkit]3568 "The name of this feature is prefixed with 'webkit' as this browser considers it experimental") | [25.0]3679 "Released on 2013-10-29.")(25.0) | 未实现 | 15.0[webkit]3568 "The name of this feature is prefixed with 'webkit' as this browser considers it experimental")<br></br>22 | 6.0[webkit]3568 "The name of this feature is prefixed with 'webkit' as this browser considers it experimental") 





## 另见<a name="另见"></a>

* [Using the Web Audio API]3811 "")



## 文档标签和贡献者
**此页面的贡献者：**[Ambar]3447 ""),[ayqy]3814 "")
**最后编辑者:**[Ambar]3447 ""),<time>Dec 12, 2016, 7:42:46 PM</time>



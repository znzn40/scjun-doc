---
manual:WebAPI
version:0
lang:zh
rawUrl:https://developer.mozilla.org/zh-CN/docs/Web/API/AudioContext
---





**`AudioContext`**接口表示由音频模块连接而成的音频处理图，每个模块对应一个[`AudioNode`]2549 "AudioNode 接口是一个处理音频的通用模块, 比如一个音频源 (e.g. 一个 HTML <audio> or <video> 元素), 一个音频地址或者一个中间处理模块 (e.g. 一个过滤器如 BiquadFilterNode, 或一个音量控制器如 GainNode).")。**`AudioContext`**可以控制它所包含的节点的创建，以及音频处理、解码操作的执行。做任何事情之前都要先创建**`AudioContext`**对象，因为一切都发生在这个环境之中。




`AudioContext`可以是事件源（event target），所以也实现了[`EventTarget`]2696 "EventTarget 是一个由可以接收事件的对象实现的接口，并且可以为它们创建侦听器。")接口。


## 属性<a name="属性"></a>
<dl><dt>[`AudioContext.currentTime`]3772 "A double.")只读</dt><dd>以双精度浮点型数字返回硬件调用的秒数，AudioContext一创建就从0开始走，无法停掉、暂停或者重置。</dd><dt>[`AudioContext.destination`]3773 "An AudioDestinationNode.")只读</dt><dd>返回[`AudioDestinationNode`]2547 "AudioDestinationNode可以通过AudioContext.destination属性来查看。")对象，表示当前audio context中所有节点的最终节点，一般表示音频渲染设备。</dd><dt>[`AudioContext.listener`]3774 "An AudioListener object.")只读</dt><dd>返回[`AudioListener`]2548 "此页面仍未被本地化, 期待您的翻译!")对象，用于3D音频空间化。</dd><dt>[`AudioContext.sampleRate`]3775 "A floating point number.")只读</dt><dd>返回用浮点数表示的采样率，也就是每秒的采样数，同一个AudioContext中的所有节点采样率相同，所以不支持采样率转换。</dd><dt>[`AudioContext.state`]3776 "DOMString，可能的值如下：")只读</dt><dd>返回`AudioContext当前状态`.</dd><dt>[`AudioContext.mozAudioChannelType`]3777 "AudioContext的mozAudioChannelType属性是只读的，在Firefox OS设备上可以用来设置音频在audio context中播放的声道。")<i></i>只读</dt><dd>被用于返回一个 Firefox OS 设备上[`AudioContext`]2544 "AudioContext接口表示由音频模块连接而成的音频处理图，每个模块对应一个AudioNode。AudioContext可以控制它所包含的节点的创建，以及音频处理、解码操作的执行。做任何事情之前都要先创建AudioContext对象，因为一切都发生在这个环境之中。")将会播放的音频声道.</dd><dt>
### 事件处理程序<a name="事件处理程序"></a>
</dt><dt>[`AudioContext.onstatechange`]3778 "下面这段代码是AudioContext states DEMO (直接运行)中的，其中onstatechange处理器会在每次当前state发生变化时把它输出到控制台。")</dt><dd>当一个`[statechange]3779 "/zh-CN/docs/Web/Reference/Events/statechange")`事件类型被触发时响应的事件处理程序.`AudioContext的状态会因`([`AudioContext.suspend`]3780 "AudioContext 接口的suspend() 方法suspends the progression of time in the audio context, temporarily halting audio hardware access and reducing CPU/battery usage in the process — this is useful if you want an application to power down the audio hardware when it will not be using an audio context for a while."),[`AudioContext.resume`]3781 "AudioContext 的 resume() 方法，恢复之前暂停播放的音频。")或者[`AudioContext.close`]3782 "AudioContext的close()方法可以关闭audio context，同时释放占用的所有系统资源。"))方法的运用而发生改变</dd></dl>
## Methods<a name="Methods"></a>


<em>可以实现</em>[`EventTarget`]2696 "EventTarget 是一个由可以接收事件的对象实现的接口，并且可以为它们创建侦听器。")接口的方法.

<dl><dt>[`AudioContext.close()`]3782 "AudioContext的close()方法可以关闭audio context，同时释放占用的所有系统资源。")</dt><dd>关闭一个音频环境, 释放任何正在使用系统资源的音频.</dd><dt>[`AudioContext.createBuffer()`]3731 "音频环境AudioContext 接口的 createBuffer() 方法用于新建一个空白的 AudioBuffer 对象，以便用于填充数据，通过 AudioBufferSourceNode 播放。")</dt><dd>创建一个空的[`AudioBuffer`]2542 "这些类型对象被设计来控制小音频片段，往往短于45秒。对于更长的声音，通过 MediaElementAudioSourceNode来实现更为合适。缓存区（buffer）包含以下数据：不间断的IEEE75432位线性PCM，从-1到1的范围额定，就是说，32位的浮点缓存区的每个样本在-1.0到1.0之间。如果AudioBuffer有不同的频道，他们通常被保存在独立的缓存区。")对象, 并且能够通过[`AudioBufferSourceNode`]2543 "AudioBufferSourceNode 接口继承自")来进行数据填充和播放.</dd><dt>[`AudioContext.createConstantSource()`]3783 "此页面仍未被本地化, 期待您的翻译!")</dt><dd>创建一个[`ConstantSourceNode`]2619 "此页面仍未被本地化, 期待您的翻译!")对象, 它持续输出一个连续的单声道，这些样本都会拥有一个相同的固定值。</dd><dt>[`AudioContext.createBufferSource()`]3784 "一个AudioBufferSourceNode对象.")</dt><dd>创建一个[`AudioBufferSourceNode`]2543 "AudioBufferSourceNode 接口继承自")对象, 他可以通过[`AudioBuffer`]2542 "这些类型对象被设计来控制小音频片段，往往短于45秒。对于更长的声音，通过 MediaElementAudioSourceNode来实现更为合适。缓存区（buffer）包含以下数据：不间断的IEEE75432位线性PCM，从-1到1的范围额定，就是说，32位的浮点缓存区的每个样本在-1.0到1.0之间。如果AudioBuffer有不同的频道，他们通常被保存在独立的缓存区。")对象来播放和处理包含在内的音频数据.[`AudioBuffer`]2542 "这些类型对象被设计来控制小音频片段，往往短于45秒。对于更长的声音，通过 MediaElementAudioSourceNode来实现更为合适。缓存区（buffer）包含以下数据：不间断的IEEE75432位线性PCM，从-1到1的范围额定，就是说，32位的浮点缓存区的每个样本在-1.0到1.0之间。如果AudioBuffer有不同的频道，他们通常被保存在独立的缓存区。")可以通过[`AudioContext.createBuffer`]3731 "音频环境AudioContext 接口的 createBuffer() 方法用于新建一个空白的 AudioBuffer 对象，以便用于填充数据，通过 AudioBufferSourceNode 播放。")方法创建或者使用[`AudioContext.decodeAudioData`]3730 "这是从音频轨道创建用于web audio API音频源的首选方法。")方法解码音轨来创建。</dd><dt>[`AudioContext.createMediaElementSource()`]3785 "为寻求更多关于媒体元素音频源节点的具体信息,请查阅 MediaElementAudioSourceNode 参考页.")</dt><dd>创建一个[`MediaElementAudioSourceNode`]2877 "MediaElementSourceNode 没有输入,只有一个输出,其由使用AudioContext.createMediaElementSource方法创建.输出的频道数目与节点创建时引用音频 HTMLMediaElement  的频道数目一致,或当 HTMLMediaElement 无音频时,频道数目为 1.")接口来关联[`HTMLMediaElement`]2770 "从父级元素 HTML 元素继承属性"). 这可以用来播放和处理来自[`<video>`]30 "HTML <video> 元素 用于在HTML或者XHTML文档中嵌入视频内容。")或[`<audio>`]148 "HTML <audio> 元素用于在文档中表示音频内容。 <audio> 元素可以包含多个音频资源， 这些音频资源可以使用 src 属性或者<source> 元素来进行描述； 浏览器将会选择最合适的一个来使用。对于不支持<audio>元素的浏览器，<audio>元素也可以作为浏览器不识别的内容加入到文档中。")元素的音频.</dd><dt>[`AudioContext.createMediaStreamSource()`]3786 "更多关于媒体流音频源(media stream audio source nodes)的细节, 请参考MediaStreamAudioSourceNode 页面.")</dt><dd>创建一个[`MediaStreamAudioSourceNode`]2896 "MediaElementSourceNode没有输入，并且只有一个输出。创建之后使用 AudioContext.createMediaStreamSource方法。输出通道的数量和AudioMediaStreamTrack的通道数量相同。如果没有有效的媒体流，输出通道就变成一个静音的通道。")接口来关联可能来自本地计算机麦克风或其他来源的音频流[`MediaStream`]2894 "MediaStream 接口是一个媒体内容的流.。一个流包含几个轨道，比如视频和音频轨道。").</dd><dt>[`AudioContext.createMediaStreamDestination()`]3787 "The MediaStream is created when the node is created and is accessible via the MediaStreamAudioDestinationNode's stream attribute. This stream can be used in a similar way as a MediaStream obtained via navigator.getUserMedia — it can, for example, be sent to a remote peer using the RTCPeerConnection addStream() method.")</dt><dd>创建一个[`MediaStreamAudioDestinationNode`]2895 "此页面仍未被本地化, 期待您的翻译!")接口来关联可能储存在本地或已发送至其他计算机的[`MediaStream`]2894 "MediaStream 接口是一个媒体内容的流.。一个流包含几个轨道，比如视频和音频轨道。")音频.</dd><dt>[`AudioContext.createScriptProcessor()`]3788 "A ScriptProcessorNode.")</dt><dd>创建一个可以通过JavaScript直接处理音频的[`ScriptProcessorNode`]3198 "").</dd><dt>[`AudioContext.createStereoPanner()`]3789 "此页面仍未被本地化, 期待您的翻译!")</dt><dd>创建一个使用立体声的音频源[`StereoPannerNode`]3227 "此页面仍未被本地化, 期待您的翻译!").</dd><dt>[`AudioContext.createAnalyser()`]3790 "AnalyserNode对象")</dt><dd>创建一个[`AnalyserNode`]2531 "AnalyserNode 赋予了节点可以提供实时频率及时间域分析的信息。它使一个 AudioNode 通过音频流不做修改的从输入到输出, 但允许你获取生成的数据, 处理它并创建音频可视化.")，它可以用来显示音频时间和频率的数据。</dd><dt>[`AudioContext.createBiquadFilter()`]3791 "一个 BiquadFilterNode.")</dt><dd>创建一个[`BiquadFilterNode`]2563 "BiquadFilterNode接口表示一个简单低阶滤波器(双二阶滤波器), 通过 AudioContext.createBiquadFilter() 方法创建. 它是一个能表示不同类型的过滤器，声调控制设备，图形均衡器的AudioNode .")，它代表代表一个双二阶滤波器，可以设置几种不同且常见滤波器类型：高通、低通、带通等。</dd><dt>[`AudioContext.createChannelMerger()`]3792 "一个 ChannelMergerNode.")</dt><dd>创建一个[`ChannelMergerNode`]2606 "此页面仍未被本地化, 期待您的翻译!")，它被用于从多个音频流信道结合成一个单一的音频流。</dd><dt>[`AudioContext.createChannelSplitter()`]3793 "一个 ChannelSplitterNode.")</dt><dd>创建一个[`ChannelSplitterNode`]2607 "此页面仍未被本地化, 期待您的翻译!")，它用于访问的音频流的单独的通道并分别对他们进行处理。</dd><dt>[`AudioContext.createConvolver()`]3794 "ConvolverNode对象。")</dt><dd>创建一个[`ConvolverNode`]2624 "此页面仍未被本地化, 期待您的翻译!")，它可用于混合效果，比如说混响效果。</dd><dt>[`AudioContext.createDelay()`]3795 "A DelayNode. The default DelayNode.delayTime if no parameter is passed to createDelay() is 0 seconds.")</dt><dd>创建一个[`DelayNode`]2661 "此页面仍未被本地化, 期待您的翻译!")，它可以通过一定量的延迟传入音频信号，它也被用做创建一个反馈回路。</dd><dt>[`AudioContext.createDynamicsCompressor()`]3796 "此页面仍未被本地化, 期待您的翻译!")</dt><dd>创建一个[`DynamicsCompressorNode`]2678 "此页面仍未被本地化, 期待您的翻译!"), 它可用于声学的音频信号的压缩。</dd><dt>[`AudioContext.createGain()`]3797 "此页面仍未被本地化, 期待您的翻译!")</dt><dd>创建一个[`GainNode`]2721 "增益是一个无单位量，会对所有输入声道的音频进行相应的增加。当对 GainNode 进行修改时，新的增益会通过 de-zippering 算法进行应用，以防止出现“咔嗒”的奇怪声响。"),它可以控制音频的总音量。</dd><dt>[`AudioContext.createIIRFilter()`]3798 "此页面仍未被本地化, 期待您的翻译!")</dt><dd>创建一个[`IIRFilterNode`]2834 "此页面仍未被本地化, 期待您的翻译!")，它可以将一个二阶滤波器配置为多种不同的通用滤波器类型。</dd><dt>[`AudioContext.createOscillator()`]3799 "此页面仍未被本地化, 期待您的翻译!")</dt><dd>创建一个[`OscillatorNode`]2975 "OscillatorNode 接口表示一个周期的波形，比如一个正弦波. 它是一个 AudioScheduledSourceNode 音频处理模块， 这个模块会对一个指定频率将创建的给定波产生影响, 一个恒定的音调."), 它表示一个周期性波形，基本上来说创造了一个音调.</dd><dt>[`AudioContext.createPanner()`]3800 "此页面仍未被本地化, 期待您的翻译!")</dt><dd>创建一个[`PannerNode`]2977 "此页面仍未被本地化, 期待您的翻译!"), 它为音源创建一个3D音源环境。</dd><dt>[`AudioContext.createPeriodicWave()`]3801 "此页面仍未被本地化, 期待您的翻译!")</dt><dd>创建一个[`PeriodicWave`]3001 "此页面仍未被本地化, 期待您的翻译!"), 创建一个用来定义[`OscillatorNode`]2975 "OscillatorNode 接口表示一个周期的波形，比如一个正弦波. 它是一个 AudioScheduledSourceNode 音频处理模块， 这个模块会对一个指定频率将创建的给定波产生影响, 一个恒定的音调.")的周期波形。</dd><dt>[`AudioContext.createWaveShaper()`]3802 "AudioContext 接口的createWaveShaper()方法创建了 表示非线性失真的WaveShaperNode。该节点通常被用来给音频添加失真效果")</dt><dd>创建一个[`WaveShaperNode`]3289 "此页面仍未被本地化, 期待您的翻译!"), 它被用于创建非线性失真效果.</dd><dt>[`AudioContext.createAudioWorker()`]3803 "此页面仍未被本地化, 期待您的翻译!")</dt><dd>创建一个[`AudioWorkerNode`]3804 "此页面仍未被本地化, 期待您的翻译!"), 它可以通过使用worker来产生，处理，或直接分析音频.</dd><dt>[`AudioContext.decodeAudioData()`]3730 "这是从音频轨道创建用于web audio API音频源的首选方法。")</dt><dd>从[`ArrayBuffer`]3805 "此页面仍未被本地化, 期待您的翻译!")对象中异步解码音频文件. 在此情况下,这个ArrayBuffer对象通常是通过使用 responseType为arraybuffer类型的[`XMLHttpRequest`]3324 "XMLHttpRequest 是一个 API，它为客户端提供了在客户端和服务器之间传输数据的功能。它提供了一个通过 URL 来获取数据的简单方式，并且不会使整个页面刷新。这使得网页只更新一部分页面而不会打扰到用户。XMLHttpRequest 在 AJAX 中被大量使用。")方法来获取的. 该方法只能作用于完整的音频文件.</dd><dt>[`AudioContext.resume()`]3781 "AudioContext 的 resume() 方法，恢复之前暂停播放的音频。")</dt><dd>重新启动一个已被暂停的音频环境</dd><dt>[`AudioContext.suspend()`]3780 "AudioContext 接口的suspend() 方法suspends the progression of time in the audio context, temporarily halting audio hardware access and reducing CPU/battery usage in the process — this is useful if you want an application to power down the audio hardware when it will not be using an audio context for a while.")</dt><dd>暂停音频内容的进度.暂时停止音频硬件访问和减少在过程中的CPU/电池使用.</dd></dl>
## 已被废弃的方法<a name="已被废弃的方法"></a>
<dl><dt>[`AudioContext.createJavaScriptNode()`]3806 "此页面仍未被本地化, 期待您的翻译!")</dt><dd>创建一个[`JavaScriptNode`]3807 "此页面仍未被本地化, 期待您的翻译!"), 用于javascript直接处理音频。 这个方法已经被[`AudioContext.createScriptProcessor()`]3788 "A ScriptProcessorNode.")替代并且废弃。</dd><dt>[`AudioContext.createWaveTable()`]3808 "此页面仍未被本地化, 期待您的翻译!")</dt><dd>创建一个[`WaveTableNode`]3809 "此页面仍未被本地化, 期待您的翻译!"), 用于顶一个周期性波形。 这个方法已经被[`AudioContext.createPeriodicWave()`]3801 "此页面仍未被本地化, 期待您的翻译!")替代并且废弃。</dd></dl>
## 例子<a name="例子"></a>


简单声明：


```
var audioCtx = new AudioContext;
```


跨浏览器的方式：


```
var audioCtx = new (window.AudioContext || window.webkitAudioContext)(); // declare new audio context
// Webkit/blink browser require a prefix, and it needs the window object specifically declared to work in Safari

var oscillatorNode = audioCtx.createOscillator();
var gainNode = audioCtx.createGain();
var finish = audioCtx.destination;
// etc.
```

## 规范<a name="规范"></a>
规范 | 状态 | 注释 
[Web Audio API<br></br><small>AudioContext</small>]3810 "") | Working Draft |  


## 浏览器兼容性<a name="浏览器兼容性"></a>


**[We&#39;re converting our compatibility data into a machine-readable JSON format]3344 "")**. This compatibility table still uses the old format, because we haven&#39;t yet converted the data it contains.**[Find out how you can help!]3392 "")**


* 
* 
Feature | Chrome | Firefox (Gecko) | Internet Explorer | Opera | Safari (WebKit) 
Basic support | 10.0[webkit]3568 "The name of this feature is prefixed with 'webkit' as this browser considers it experimental") | [25.0]3679 "Released on 2013-10-29.")(25.0) | 未实现 | 15.0[webkit]3568 "The name of this feature is prefixed with 'webkit' as this browser considers it experimental") | 6.0[webkit]3568 "The name of this feature is prefixed with 'webkit' as this browser considers it experimental") 





## 另见<a name="另见"></a>

* [Using the Web Audio API]3811 "")



## 文档标签和贡献者
**此页面的贡献者：**[Jack_lo]3812 ""),[maicss]3444 ""),[yqjiang]3813 ""),[Taoja]3471 ""),[ayqy]3814 ""),[focus]3815 "")
**最后编辑者:**[Jack_lo]3812 ""),<time>Sep 11, 2017, 6:48:28 PM</time>



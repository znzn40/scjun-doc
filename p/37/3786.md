---
manual:WebAPI
version:0
lang:zh
rawUrl:https://developer.mozilla.org/zh-CN/docs/Web/API/AudioContext/createMediaStreamSource
---



这篇翻译不完整。请帮忙从英语[翻译这篇文章]22898 "")。






[`AudioContext`]2544 "AudioContext接口表示由音频模块连接而成的音频处理图，每个模块对应一个AudioNode。AudioContext可以控制它所包含的节点的创建，以及音频处理、解码操作的执行。做任何事情之前都要先创建AudioContext对象，因为一切都发生在这个环境之中。")接口的`createMediaStreamSource()方法用于创建一个新的`[`MediaStreamAudioSourceNode`]2896 "MediaElementSourceNode没有输入，并且只有一个输出。创建之后使用 AudioContext.createMediaStreamSource方法。输出通道的数量和AudioMediaStreamTrack的通道数量相同。如果没有有效的媒体流，输出通道就变成一个静音的通道。")对象, 需要传入一个媒体流对象(MediaStream对象)(可以从[`navigator.getUserMedia`]14971 "Navigator.getUserMedia()方法提醒用户需要使用音频（0或者1）和（0或者1）视频输入设备，比如相机，屏幕共享，或者麦克风。如果用户给予许可，successCallback回调就会被调用，MediaStream对象作为回调函数的参数。如果用户拒绝许可或者没有媒体可用，errorCallback就会被调用，类似的，PermissionDeniedError 或者NotFoundError对象作为它的参数。注意，有可能以上两个回调函数都不被调用，因为不要求用户一定作出选择（允许或者拒绝）。")获得MediaStream对象实例), 然后来自MediaStream的音频就可以被播放和操作。




更多关于媒体流音频源(media stream audio source nodes)的细节, 请参考[`MediaStreamAudioSourceNode`]2896 "MediaElementSourceNode没有输入，并且只有一个输出。创建之后使用 AudioContext.createMediaStreamSource方法。输出通道的数量和AudioMediaStreamTrack的通道数量相同。如果没有有效的媒体流，输出通道就变成一个静音的通道。")页面.


## 语法<a name="语法"></a>

```
var audioCtx = new AudioContext();
var source = audioCtx.createMediaStreamSource(stream);
```

### 参数<a name="参数"></a>
<dl><dt id=''>stream</dt><dd>一个[`MediaStream`]2894 "MediaStream 接口是一个媒体内容的流.。一个流包含几个轨道，比如视频和音频轨道。")对象，把他传入一个音频处理器进行操作</dd></dl>
### Returns<a name="Returns"></a>


[`MediaStreamAudioSourceNode`]2896 "MediaElementSourceNode没有输入，并且只有一个输出。创建之后使用 AudioContext.createMediaStreamSource方法。输出通道的数量和AudioMediaStreamTrack的通道数量相同。如果没有有效的媒体流，输出通道就变成一个静音的通道。")


## 示例<a name="示例"></a>


本例中，我们从[`navigator.getUserMedia`]14971 "Navigator.getUserMedia()方法提醒用户需要使用音频（0或者1）和（0或者1）视频输入设备，比如相机，屏幕共享，或者麦克风。如果用户给予许可，successCallback回调就会被调用，MediaStream对象作为回调函数的参数。如果用户拒绝许可或者没有媒体可用，errorCallback就会被调用，类似的，PermissionDeniedError 或者NotFoundError对象作为它的参数。注意，有可能以上两个回调函数都不被调用，因为不要求用户一定作出选择（允许或者拒绝）。")获取媒体 (audio + video) 流,，把它传入[`<video>`]30 "HTML <video> 元素 用于在HTML或者XHTML文档中嵌入视频内容。")中播放，并把视频调成静音，然后把获取到的audio传入[`MediaStreamAudioSourceNode`]2896 "MediaElementSourceNode没有输入，并且只有一个输出。创建之后使用 AudioContext.createMediaStreamSource方法。输出通道的数量和AudioMediaStreamTrack的通道数量相同。如果没有有效的媒体流，输出通道就变成一个静音的通道。")。接下来我们把获取到的audio传入[`BiquadFilterNode`]2563 "BiquadFilterNode接口表示一个简单低阶滤波器(双二阶滤波器), 通过 AudioContext.createBiquadFilter() 方法创建. 它是一个能表示不同类型的过滤器，声调控制设备，图形均衡器的AudioNode .")(可以把声音转化为低音)，输出到[`AudioDestinationNode`]2547 "AudioDestinationNode可以通过AudioContext.destination属性来查看。").



[`<video>`]30 "HTML <video> 元素 用于在HTML或者XHTML文档中嵌入视频内容。")元素下面滑动杆控制低音过滤器过滤的程度，滑动杆的值越大，低音更明显



**注意：你可以查看**[在线演示]14972 "")，或者[查看源码]14973 "")。



```
var pre = document.querySelector('pre');
var video = document.querySelector('video');
var myScript = document.querySelector('script');
var range = document.querySelector('input');

// getUserMedia获取流
// 把流放入MediaStreamAudioSourceNode
// 输出到video元素

if (navigator.mediaDevices) {
    console.log('getUserMedia supported.');
    navigator.mediaDevices.getUserMedia ({audio: true, video: true})
    .then(function(stream) {
        video.srcObject = stream;
        video.onloadedmetadata = function(e) {
            video.play();
            video.muted = true;
        };

        // 创建MediaStreamAudioSourceNode
        // Feed the HTMLMediaElement into it
        var audioCtx = new AudioContext();
        var source = audioCtx.createMediaStreamSource(stream);

        // 创建二阶滤波器
        var biquadFilter = audioCtx.createBiquadFilter();
        biquadFilter.type = "lowshelf";
        biquadFilter.frequency.value = 1000;
        biquadFilter.gain.value = range.value;

        // 把AudioBufferSourceNode连接到gainNode
        // gainNode连接到目的地, 所以我们可以播放
        // 音乐并用鼠标调节音量
        source.connect(biquadFilter);
        biquadFilter.connect(audioCtx.destination);

        // Get new mouse pointer coordinates when mouse is moved
        // then set new gain value

        range.oninput = function() {
            biquadFilter.gain.value = range.value;
        }
    })
    .catch(function(err) {
        console.log('The following gUM error occured: ' + err);
    });
} else {
   console.log('getUserMedia not supported on your browser!');
}

// dump script to pre element

pre.innerHTML = myScript.innerHTML; 

```


**注意**: 调用`createMediaStreamSource()`, 来自于媒体流的音频回放将被重新传到AudioContext的处理器中。所以播放/暂停流仍然是可以通过media元素的API和自带的控制器控制。







## 规范<a name="规范"></a>
Specification | Status | Comment 
[Web Audio API<br></br><small>createMediaStreamSource()</small>]22899 "") | Working Draft |  


## 浏览器兼容性<a name="浏览器兼容性"></a>


**[We&#39;re converting our compatibility data into a machine-readable JSON format]3344 "")**. This compatibility table still uses the old format, because we haven&#39;t yet converted the data it contains.**[Find out how you can help!]3392 "")**


* 
* 
Feature | Chrome | Edge | Firefox (Gecko) | Internet Explorer | Opera | Safari (WebKit) 
Basic support | 10.0[webkit]3568 "The name of this feature is prefixed with 'webkit' as this browser considers it experimental") | (Yes) | [25.0]3679 "Released on 2013-10-29.")(25.0) | 未实现 | 15.0[webkit]3568 "The name of this feature is prefixed with 'webkit' as this browser considers it experimental")<br></br>22 (unprefixed) | 6.0[webkit]3568 "The name of this feature is prefixed with 'webkit' as this browser considers it experimental") 





## 查看更多<a name="查看更多"></a>

* [Using the Web Audio API]3811 "")



## 文档标签和贡献者
**此页面的贡献者：**[maohhgg]3746 ""),[Remond]22900 "")
**最后编辑者:**[maohhgg]3746 ""),<time>Mar 2, 2017, 9:52:59 AM</time>


